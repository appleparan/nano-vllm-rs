//! Rejection sampler for speculative decoding.
//!
//! Implements the token verification algorithm that ensures the output
//! distribution matches the target model exactly.

use candle_core::{D, IndexOp, Result, Tensor};
use rand::distributions::Distribution;
use rand::{Rng, SeedableRng};

/// Rejection sampler for speculative decoding.
///
/// Verifies draft tokens against target model distribution and decides
/// which tokens to accept or reject.
///
/// ## Algorithm
///
/// For each draft token `t` at position `i`:
///
/// 1. Compute acceptance probability: `α = min(1, P_target[t] / P_draft[t])`
/// 2. Sample `u ~ Uniform(0, 1)`
/// 3. If `u < α`: accept token, continue to next
/// 4. Else: reject token, resample from adjusted distribution
///
/// The adjusted distribution ensures correctness:
/// `P_adjusted = max(0, P_target - P_draft) / Z`
///
/// where `Z` is the normalization constant.
#[derive(Debug)]
pub struct RejectionSampler {
    rng: rand::rngs::StdRng,
}

impl RejectionSampler {
    /// Create a new rejection sampler with random seed.
    pub fn new() -> Self {
        Self {
            rng: rand::rngs::StdRng::from_entropy(),
        }
    }

    /// Create a new rejection sampler with a specific seed for reproducibility.
    pub fn with_seed(seed: u64) -> Self {
        Self {
            rng: rand::rngs::StdRng::seed_from_u64(seed),
        }
    }

    /// Verify draft tokens against target distribution.
    ///
    /// # Arguments
    ///
    /// * `draft_tokens` - Token IDs generated by draft model
    /// * `draft_logits` - Logits from draft model [K, vocab_size]
    /// * `target_logits` - Logits from target model [K+1, vocab_size]
    /// * `temperature` - Sampling temperature (applied to both distributions)
    ///
    /// # Returns
    ///
    /// `(accepted_tokens, final_token, num_accepted)`:
    /// - `accepted_tokens`: Draft tokens that were accepted
    /// - `final_token`: Final token (either from acceptance or resampling)
    /// - `num_accepted`: Number of draft tokens accepted (0 to K)
    pub fn verify(
        &mut self,
        draft_tokens: &[u32],
        draft_logits: &Tensor,
        target_logits: &Tensor,
        temperature: f32,
    ) -> Result<(Vec<u32>, u32, usize)> {
        let k = draft_tokens.len();

        // Validate shapes
        let draft_shape = draft_logits.dims();
        let target_shape = target_logits.dims();

        if draft_shape.len() != 2 || target_shape.len() != 2 {
            return Err(candle_core::Error::Msg(format!(
                "Expected 2D tensors, got draft: {draft_shape:?}, target: {target_shape:?}"
            )));
        }

        if draft_shape[0] != k {
            return Err(candle_core::Error::Msg(format!(
                "Draft logits first dim {} != num draft tokens {}",
                draft_shape[0], k
            )));
        }

        if target_shape[0] != k + 1 {
            return Err(candle_core::Error::Msg(format!(
                "Target logits first dim {} != K+1 {}",
                target_shape[0],
                k + 1
            )));
        }

        let mut accepted = Vec::with_capacity(k);

        // Apply temperature and convert to probabilities
        let draft_probs = self.logits_to_probs(draft_logits, temperature)?;
        let target_probs = self.logits_to_probs(target_logits, temperature)?;

        for (i, &draft_token) in draft_tokens.iter().enumerate() {
            // Get probabilities for the draft token
            let p_draft = self.get_prob(&draft_probs, i, draft_token)?;
            let p_target = self.get_prob(&target_probs, i, draft_token)?;

            // Acceptance probability: α = min(1, P_target / P_draft)
            let alpha = if p_draft > 0.0 {
                (p_target / p_draft).min(1.0)
            } else {
                // If draft probability is 0, always reject
                0.0
            };

            // Sample u ~ Uniform(0, 1)
            let u: f32 = self.rng.r#gen();

            if u < alpha {
                // Accept the draft token
                accepted.push(draft_token);
            } else {
                // Reject: sample from adjusted distribution
                let final_token = self.sample_adjusted(&target_probs, &draft_probs, i)?;
                let num_accepted = accepted.len();
                return Ok((accepted, final_token, num_accepted));
            }
        }

        // All K tokens accepted, sample K+1 from target
        let final_token = self.sample_from_probs(&target_probs, k)?;
        Ok((accepted, final_token, k))
    }

    /// Convert logits to probabilities with temperature scaling.
    fn logits_to_probs(&self, logits: &Tensor, temperature: f32) -> Result<Tensor> {
        let scaled = if temperature != 1.0 && temperature > 0.0 {
            (logits / temperature as f64)?
        } else if temperature == 0.0 {
            // For greedy, we'll handle this separately
            logits.clone()
        } else {
            logits.clone()
        };

        // Apply softmax
        candle_nn::ops::softmax(&scaled, D::Minus1)
    }

    /// Get probability of a specific token at a specific position.
    fn get_prob(&self, probs: &Tensor, position: usize, token: u32) -> Result<f32> {
        let row = probs.i(position)?;
        let p = row.i(token as usize)?.to_scalar::<f32>()?;
        Ok(p)
    }

    /// Sample from adjusted distribution: P_adjusted = max(0, P_target - P_draft) / Z
    fn sample_adjusted(
        &mut self,
        target_probs: &Tensor,
        draft_probs: &Tensor,
        position: usize,
    ) -> Result<u32> {
        let target_row = target_probs.i(position)?;
        let draft_row = draft_probs.i(position)?;

        // Compute adjusted distribution: max(0, P_target - P_draft)
        let adjusted = target_row.sub(&draft_row)?;
        let adjusted = adjusted.maximum(&Tensor::zeros_like(&adjusted)?)?;

        // Normalize
        let sum = adjusted.sum_all()?.to_scalar::<f32>()?;

        if sum <= 0.0 {
            // Fallback to target distribution if adjusted is all zeros
            return self.sample_from_probs(target_probs, position);
        }

        let normalized = (&adjusted / sum as f64)?;

        // Sample
        self.sample_from_tensor(&normalized)
    }

    /// Sample a token from probability distribution at given position.
    fn sample_from_probs(&mut self, probs: &Tensor, position: usize) -> Result<u32> {
        let row = probs.i(position)?;
        self.sample_from_tensor(&row)
    }

    /// Sample a token from a 1D probability tensor.
    fn sample_from_tensor(&mut self, probs: &Tensor) -> Result<u32> {
        let probs_vec: Vec<f32> = probs.to_vec1()?;

        // Renormalize to ensure sum = 1
        let sum: f32 = probs_vec.iter().sum();
        let normalized: Vec<f64> = probs_vec.iter().map(|&p| (p / sum) as f64).collect();

        // Filter out invalid probabilities
        let normalized: Vec<f64> = normalized
            .iter()
            .map(|&p| if p.is_nan() || p < 0.0 { 0.0 } else { p })
            .collect();

        let sum: f64 = normalized.iter().sum();
        if sum <= 0.0 {
            // If all probabilities are zero/invalid, return first token
            return Ok(0);
        }

        let normalized: Vec<f64> = normalized.iter().map(|&p| p / sum).collect();

        // Sample using weighted distribution
        let dist = rand::distributions::WeightedIndex::new(&normalized)
            .map_err(|e| candle_core::Error::Msg(format!("Failed to create distribution: {e}")))?;

        let sampled_idx = dist.sample(&mut self.rng);
        Ok(sampled_idx as u32)
    }
}

impl Default for RejectionSampler {
    fn default() -> Self {
        Self::new()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use candle_core::Device;

    fn create_uniform_logits(k: usize, vocab_size: usize, device: &Device) -> Result<Tensor> {
        // Uniform logits (all zeros -> uniform after softmax)
        Tensor::zeros((k, vocab_size), candle_core::DType::F32, device)
    }

    #[test]
    fn test_sampler_creation() {
        let sampler = RejectionSampler::new();
        assert!(format!("{sampler:?}").contains("RejectionSampler"));

        let seeded = RejectionSampler::with_seed(42);
        assert!(format!("{seeded:?}").contains("RejectionSampler"));
    }

    #[test]
    fn test_accept_all_when_same_distribution() {
        let device = Device::Cpu;
        let mut sampler = RejectionSampler::with_seed(42);

        let k = 4;
        let vocab_size = 100;

        // Same distribution for draft and target -> high acceptance rate
        let draft_logits = create_uniform_logits(k, vocab_size, &device).unwrap();
        let target_logits = create_uniform_logits(k + 1, vocab_size, &device).unwrap();

        // Draft tokens (any valid tokens)
        let draft_tokens: Vec<u32> = vec![10, 20, 30, 40];

        let (accepted, final_token, num_accepted) = sampler
            .verify(&draft_tokens, &draft_logits, &target_logits, 1.0)
            .unwrap();

        // With uniform distribution, P_target = P_draft for all tokens
        // So α = min(1, 1) = 1, meaning 100% acceptance
        assert_eq!(num_accepted, k);
        assert_eq!(accepted.len(), k);
        assert!(final_token < vocab_size as u32);
    }

    #[test]
    fn test_reject_when_target_prefers_different() {
        let device = Device::Cpu;
        let mut sampler = RejectionSampler::with_seed(42);

        let k = 1;

        // Draft strongly prefers token 0
        let draft_logits = Tensor::new(
            &[[10.0f32, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]],
            &device,
        )
        .unwrap();

        // Target strongly prefers token 5
        let target_logits = Tensor::new(
            &[
                [0.0f32, 0.0, 0.0, 0.0, 0.0, 10.0, 0.0, 0.0, 0.0, 0.0],
                [0.0f32, 0.0, 0.0, 0.0, 0.0, 10.0, 0.0, 0.0, 0.0, 0.0],
            ],
            &device,
        )
        .unwrap();

        // Draft generated token 0 (which target doesn't like)
        let draft_tokens: Vec<u32> = vec![0];

        let (accepted, final_token, num_accepted) = sampler
            .verify(&draft_tokens, &draft_logits, &target_logits, 1.0)
            .unwrap();

        // Target probability for token 0 is very low -> likely rejection
        // After rejection, should resample from adjusted distribution
        // which heavily favors token 5
        assert!(num_accepted <= k);
        assert!(accepted.len() == num_accepted);
        // Verify the function returns valid results
        assert!(final_token < 10);
    }

    #[test]
    fn test_shape_validation() {
        let device = Device::Cpu;
        let mut sampler = RejectionSampler::new();

        // Mismatched shapes
        let draft_logits = Tensor::zeros((3, 10), candle_core::DType::F32, &device).unwrap();
        let target_logits = Tensor::zeros((3, 10), candle_core::DType::F32, &device).unwrap(); // Should be 4

        let draft_tokens: Vec<u32> = vec![1, 2, 3];

        let result = sampler.verify(&draft_tokens, &draft_logits, &target_logits, 1.0);
        assert!(result.is_err());
    }

    #[test]
    fn test_temperature_effect() {
        let device = Device::Cpu;

        let k = 2;

        // Create logits with some variation
        let draft_logits = Tensor::new(&[[1.0f32; 10], [1.0f32; 10]], &device).unwrap();
        let target_logits =
            Tensor::new(&[[1.0f32; 10], [1.0f32; 10], [1.0f32; 10]], &device).unwrap();

        let draft_tokens: Vec<u32> = vec![0, 1];

        // Low temperature (more deterministic)
        let mut sampler_low = RejectionSampler::with_seed(123);
        let (_, _, num_low) = sampler_low
            .verify(&draft_tokens, &draft_logits, &target_logits, 0.1)
            .unwrap();

        // High temperature (more random)
        let mut sampler_high = RejectionSampler::with_seed(123);
        let (_, _, num_high) = sampler_high
            .verify(&draft_tokens, &draft_logits, &target_logits, 2.0)
            .unwrap();

        // Both should work without errors
        assert!(num_low <= k);
        assert!(num_high <= k);
    }

    #[test]
    fn test_reproducibility_with_seed() {
        let device = Device::Cpu;

        let k = 4;
        let vocab_size = 50;

        let draft_logits = Tensor::randn(0.0f32, 1.0, (k, vocab_size), &device).unwrap();
        let target_logits = Tensor::randn(0.0f32, 1.0, (k + 1, vocab_size), &device).unwrap();

        let draft_tokens: Vec<u32> = vec![5, 10, 15, 20];

        // Same seed should give same results
        let mut sampler1 = RejectionSampler::with_seed(42);
        let mut sampler2 = RejectionSampler::with_seed(42);

        let result1 = sampler1
            .verify(&draft_tokens, &draft_logits, &target_logits, 1.0)
            .unwrap();
        let result2 = sampler2
            .verify(&draft_tokens, &draft_logits, &target_logits, 1.0)
            .unwrap();

        assert_eq!(result1.0, result2.0);
        assert_eq!(result1.1, result2.1);
        assert_eq!(result1.2, result2.2);
    }
}
