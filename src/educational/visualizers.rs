//! ASCII art generators for educational visualizations.
//!
//! This module provides ASCII-based visualizations for understanding
//! LLM inference, including attention heatmaps, memory usage bars,
//! probability distributions, and architectural diagrams.

/// Unicode block characters for different intensities.
const BLOCKS: [char; 5] = [' ', 'â–‘', 'â–’', 'â–“', 'â–ˆ'];

/// Convert a value [0, max_val] to a block character.
fn intensity_to_block(value: f32, max_val: f32) -> char {
    if max_val == 0.0 {
        return BLOCKS[0];
    }
    let normalized = (value / max_val).clamp(0.0, 1.0);
    let idx = (normalized * (BLOCKS.len() - 1) as f32) as usize;
    BLOCKS[idx.min(BLOCKS.len() - 1)]
}

/// Generate ASCII heatmap of attention weights.
///
/// # Arguments
///
/// * `weights` - 2D slice of attention weights [query_len, key_len]
/// * `row_labels` - Labels for rows (query tokens)
/// * `col_labels` - Labels for columns (key tokens)
/// * `max_label_width` - Maximum width for token labels
/// * `title` - Optional title for the heatmap
///
/// # Returns
///
/// ASCII art string representing the attention heatmap.
pub fn attention_heatmap_ascii(
    weights: &[Vec<f32>],
    row_labels: &[String],
    col_labels: &[String],
    max_label_width: usize,
    title: Option<&str>,
) -> String {
    let mut lines = Vec::new();

    // Truncate labels
    let row_labels: Vec<String> = row_labels
        .iter()
        .map(|l| {
            let truncated: String = l.chars().take(max_label_width).collect();
            format!("{truncated:max_label_width$}")
        })
        .collect();
    let col_labels: Vec<String> = col_labels
        .iter()
        .map(|l| l.chars().take(3).collect())
        .collect();

    // Title
    if let Some(t) = title {
        lines.push(t.to_string());
        lines.push("â”€".repeat(max_label_width + 2 + col_labels.len() * 4));
    }

    // Header row (column labels)
    let mut header = " ".repeat(max_label_width + 2);
    for label in &col_labels {
        header.push_str(&format!(" {label} "));
    }
    lines.push(header);

    // Data rows
    for (i, row_label) in row_labels.iter().enumerate() {
        let mut row_str = format!("{row_label}  ");
        for j in 0..col_labels.len() {
            if i < weights.len() && j < weights[i].len() {
                let val = weights[i][j];
                let block = intensity_to_block(val, 1.0);
                row_str.push_str(&format!(" {block}{block}{block}"));
            } else {
                row_str.push_str("    ");
            }
        }
        lines.push(row_str);
    }

    lines.join("\n")
}

/// Generate memory usage bar.
///
/// # Arguments
///
/// * `used` - Used memory/blocks
/// * `total` - Total memory/blocks
/// * `width` - Bar width in characters
/// * `label` - Optional label prefix
/// * `show_percentage` - Show percentage at end
///
/// # Returns
///
/// ASCII bar string.
pub fn memory_bar(
    used: usize,
    total: usize,
    width: usize,
    label: &str,
    show_percentage: bool,
) -> String {
    let pct = if total == 0 {
        0.0
    } else {
        used as f32 / total as f32
    };

    let filled = (pct * width as f32) as usize;
    let empty = width - filled;

    let bar = format!("{}{}", "â–ˆ".repeat(filled), "â–‘".repeat(empty));

    let mut result = format!("{label}{bar}");
    if show_percentage {
        result.push_str(&format!(" {:.0}%", pct * 100.0));
    }

    result
}

/// Generate probability distribution bars.
///
/// # Arguments
///
/// * `probs` - List of probabilities
/// * `labels` - List of labels
/// * `max_width` - Maximum bar width
/// * `top_k` - Number of top items to show
///
/// # Returns
///
/// ASCII bars showing probability distribution.
pub fn probability_bars(
    probs: &[f32],
    labels: &[String],
    max_width: usize,
    top_k: usize,
) -> String {
    // Sort by probability
    let mut items: Vec<(f32, &String)> = probs.iter().copied().zip(labels.iter()).collect();
    items.sort_by(|a, b| b.0.partial_cmp(&a.0).unwrap_or(std::cmp::Ordering::Equal));
    let items: Vec<_> = items.into_iter().take(top_k).collect();

    let max_label_len = items.iter().map(|(_, l)| l.len()).max().unwrap_or(0);

    let mut lines = Vec::new();

    for (prob, label) in &items {
        let bar_len = (prob * max_width as f32) as usize;
        let bar = "â–ˆ".repeat(bar_len);
        let padded_label = format!("{label:max_label_len$}");
        lines.push(format!(
            "â”‚ {} â”‚ {:5.1}%  {}",
            padded_label,
            prob * 100.0,
            bar
        ));
    }

    // Create table border
    let border_len = max_label_len + 2;
    let header = format!("â”Œ{}â”¬{}â”", "â”€".repeat(border_len), "â”€".repeat(10));
    let footer = format!("â””{}â”´{}â”˜", "â”€".repeat(border_len), "â”€".repeat(10));

    format!("{}\n{}\n{}", header, lines.join("\n"), footer)
}

/// Display tokens in a box with optional highlighting.
///
/// # Arguments
///
/// * `tokens` - List of token strings
/// * `highlight_idx` - Index of token to highlight with cursor
/// * `prefix` - Optional prefix text
///
/// # Returns
///
/// Boxed token sequence.
pub fn token_sequence_box(tokens: &[String], highlight_idx: Option<usize>, prefix: &str) -> String {
    if tokens.is_empty() {
        return "â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n\
                â”‚ (empty)                                  â”‚\n\
                â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯"
            .to_string();
    }

    // Build token string with brackets
    let token_strs: Vec<String> = tokens
        .iter()
        .enumerate()
        .map(|(i, tok)| {
            if highlight_idx == Some(i) {
                format!("[{tok}]")
            } else {
                tok.clone()
            }
        })
        .collect();

    let mut content = token_strs.join(" ");
    if highlight_idx == Some(tokens.len()) {
        content.push('â–ˆ'); // Cursor at end
    }

    // Box it
    let width = 40.max(content.len() + prefix.len() + 4);
    let inner_width = width - 2;
    let content_padded = format!(
        "{}{:width$}",
        prefix,
        content,
        width = inner_width - prefix.len()
    );

    format!(
        "â•­{}â•®\nâ”‚ {} â”‚\nâ•°{}â•¯",
        "â”€".repeat(inner_width),
        content_padded,
        "â”€".repeat(inner_width)
    )
}

/// Return the full model architecture ASCII diagram.
pub fn model_architecture_diagram() -> &'static str {
    r#"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LLaMA Architecture                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Input: "The capital of France is"                              â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚
â”‚  â”‚   Tokenizer     â”‚  "The" â†’ 450, "capital" â†’ 7483, ...        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                            â”‚
â”‚           â–¼                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚
â”‚  â”‚   Embedding     â”‚  450 â†’ [0.12, -0.34, 0.87, ...]  (4096-d)  â”‚
â”‚  â”‚   (Lookup)      â”‚                                            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                            â”‚
â”‚           â–¼                                                     â”‚
â”‚  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—                                            â”‚
â”‚  â•‘  Decoder Layer  â•‘ Ã—32                                        â”‚
â”‚  â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘                                            â”‚
â”‚  â•‘  â”‚ RMSNorm   â”‚  â•‘                                            â”‚
â”‚  â•‘  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â•‘                                            â”‚
â”‚  â•‘        â–¼        â•‘                                            â”‚
â”‚  â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘                                            â”‚
â”‚  â•‘  â”‚ Attention â”‚â—„â”€â•¬â”€â”€â”€â”€ KV Cache (stores past K,V)             â”‚
â”‚  â•‘  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â•‘                                            â”‚
â”‚  â•‘        â”‚+       â•‘  â—„â”€â”€ Residual connection                   â”‚
â”‚  â•‘        â–¼        â•‘                                            â”‚
â”‚  â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘                                            â”‚
â”‚  â•‘  â”‚ RMSNorm   â”‚  â•‘                                            â”‚
â”‚  â•‘  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â•‘                                            â”‚
â”‚  â•‘        â–¼        â•‘                                            â”‚
â”‚  â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘                                            â”‚
â”‚  â•‘  â”‚    FFN    â”‚  â•‘  (SwiGLU: up_proj, gate, down_proj)        â”‚
â”‚  â•‘  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â•‘                                            â”‚
â”‚  â•‘        â”‚+       â•‘  â—„â”€â”€ Residual connection                   â”‚
â”‚  â•šâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•                                            â”‚
â”‚           â–¼                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚
â”‚  â”‚   Final Norm    â”‚                                            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                            â”‚
â”‚           â–¼                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚
â”‚  â”‚    LM Head      â”‚  Project to vocabulary (32000 tokens)      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                            â”‚
â”‚           â–¼                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚
â”‚  â”‚    Softmax      â”‚  â†’ Probability distribution                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                            â”‚
â”‚           â–¼                                                     â”‚
â”‚      "Paris" (87%)                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"#
}

/// Return the attention mechanism ASCII diagram.
pub fn attention_mechanism_diagram() -> &'static str {
    r#"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Self-Attention Explained                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Input: [The] [capital] [of] [France] [is]                      â”‚
â”‚                                                                 â”‚
â”‚  Step 1: Create Q, K, V for each token                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â”‚
â”‚         â”Œâ”€â”€â”€â”     â”Œâ”€â”€â”€â”     â”Œâ”€â”€â”€â”                               â”‚
â”‚  The â”€â”€â–ºâ”‚ Q â”‚     â”‚ K â”‚     â”‚ V â”‚                               â”‚
â”‚         â””â”€â”€â”€â”˜     â””â”€â”€â”€â”˜     â””â”€â”€â”€â”˜                               â”‚
â”‚         â”Œâ”€â”€â”€â”     â”Œâ”€â”€â”€â”     â”Œâ”€â”€â”€â”                               â”‚
â”‚  cap â”€â”€â–ºâ”‚ Q â”‚     â”‚ K â”‚     â”‚ V â”‚     Q = "What am I looking for?"
â”‚         â””â”€â”€â”€â”˜     â””â”€â”€â”€â”˜     â””â”€â”€â”€â”˜     K = "What do I contain?"  â”‚
â”‚         â”Œâ”€â”€â”€â”     â”Œâ”€â”€â”€â”     â”Œâ”€â”€â”€â”     V = "What info do I give?"â”‚
â”‚  of  â”€â”€â–ºâ”‚ Q â”‚     â”‚ K â”‚     â”‚ V â”‚                               â”‚
â”‚         â””â”€â”€â”€â”˜     â””â”€â”€â”€â”˜     â””â”€â”€â”€â”˜                               â”‚
â”‚          ...       ...       ...                                â”‚
â”‚                                                                 â”‚
â”‚  Step 2: Compute attention scores (Q @ K^T)                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                     â”‚
â”‚                                                                 â”‚
â”‚            Keys:  The  cap   of  Fra   is                       â”‚
â”‚  Queries:      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚     The        â”‚ 0.8  0.1  0.0  0.0  0.0    â”‚  Can only see     â”‚
â”‚     capital    â”‚ 0.3  0.6  0.0  0.0  0.0    â”‚  itself & past    â”‚
â”‚     of         â”‚ 0.1  0.4  0.4  0.0  0.0    â”‚  (causal mask!)   â”‚
â”‚     France     â”‚ 0.0  0.5  0.1  0.3  0.0    â”‚                   â”‚
â”‚     is         â”‚ 0.0  0.2  0.0  0.7  0.1    â”‚                   â”‚
â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                  â–²                                              â”‚
â”‚                  â”‚ Higher = pays more attention                 â”‚
â”‚                                                                 â”‚
â”‚  Step 3: Weighted sum of Values                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                  â”‚
â”‚     output["is"] = 0.2Ã—V[cap] + 0.7Ã—V[Fra] + 0.1Ã—V[is]          â”‚
â”‚                                                                 â”‚
â”‚     "is" pays most attention to "France" - makes sense!         â”‚
â”‚     This helps it predict "Paris" next.                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"#
}

/// Return the KV cache explanation diagram.
pub fn kv_cache_diagram() -> &'static str {
    r#"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     KV Cache: Why We Cache                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  WITHOUT CACHE (Quadratic complexity):                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                         â”‚
â”‚  Step 1: Process [The]                    â†’ Compute K,V for 1   â”‚
â”‚  Step 2: Process [The][capital]           â†’ Compute K,V for 2   â”‚
â”‚  Step 3: Process [The][capital][of]       â†’ Compute K,V for 3   â”‚
â”‚  Step 4: Process [The][capital][of][France] â†’ Compute K,V for 4 â”‚
â”‚                                                                 â”‚
â”‚  Total K,V computations: 1+2+3+4 = 10 (O(nÂ²) for n tokens!)     â”‚
â”‚                                                                 â”‚
â”‚  WITH CACHE (Linear complexity):                                â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                               â”‚
â”‚                                                                 â”‚
â”‚  Prefill: Compute K,V for all prompt tokens at once             â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Layer 1 Cache    â”‚ Kâ‚ â”‚ Kâ‚‚ â”‚ Kâ‚ƒ â”‚ Kâ‚„ â”‚ Kâ‚… â”‚    â”‚    â”‚ â”‚    â”‚
â”‚  â”‚                  â”‚ Vâ‚ â”‚ Vâ‚‚ â”‚ Vâ‚ƒ â”‚ Vâ‚„ â”‚ Vâ‚… â”‚    â”‚    â”‚ â”‚    â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”¤    â”‚
â”‚  â”‚ Layer 2 Cache    â”‚ Kâ‚ â”‚ Kâ‚‚ â”‚ Kâ‚ƒ â”‚ Kâ‚„ â”‚ Kâ‚… â”‚    â”‚    â”‚ â”‚    â”‚
â”‚  â”‚                  â”‚ Vâ‚ â”‚ Vâ‚‚ â”‚ Vâ‚ƒ â”‚ Vâ‚„ â”‚ Vâ‚… â”‚    â”‚    â”‚ â”‚    â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”¤    â”‚
â”‚  â”‚ ...              â”‚    â”‚    â”‚    â”‚    â”‚    â”‚    â”‚    â”‚ â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                   â–²                             â”‚
â”‚                                   â””â”€â”€ Empty slots for decode    â”‚
â”‚                                                                 â”‚
â”‚  Decode: Only compute K,V for the NEW token                     â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Layer 1 Cache    â”‚ Kâ‚ â”‚ Kâ‚‚ â”‚ Kâ‚ƒ â”‚ Kâ‚„ â”‚ Kâ‚… â”‚ Kâ‚† â”‚    â”‚ â”‚    â”‚
â”‚  â”‚                  â”‚ Vâ‚ â”‚ Vâ‚‚ â”‚ Vâ‚ƒ â”‚ Vâ‚„ â”‚ Vâ‚… â”‚ Vâ‚† â”‚    â”‚ â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                            â–²                    â”‚
â”‚                                            â””â”€â”€ New token added! â”‚
â”‚                                                                 â”‚
â”‚  Total K,V computations: 5 + 1 + 1 + 1 = 8 (O(n) linear!)       â”‚
â”‚                                                                 â”‚
â”‚  Memory: 5 tokens Ã— 32 layers Ã— 2 (K,V) Ã— 4096 dims Ã— 2 bytes   â”‚
â”‚        = 2.6 MB for just 5 tokens!                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"#
}

/// Return the PagedAttention explanation diagram.
pub fn paged_attention_diagram() -> &'static str {
    r#"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PagedAttention Explained                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  PROBLEM: Sequences have variable lengths                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                      â”‚
â”‚                                                                 â”‚
â”‚  Seq A: "Hello"           (1 token)                             â”‚
â”‚  Seq B: "The quick brown fox jumps over the lazy dog" (9 tokens)â”‚
â”‚                                                                 â”‚
â”‚  Traditional: Pre-allocate max_length for each â†’ WASTE!         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ Seq A: â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  (1/32 used) â”‚        â”‚
â”‚  â”‚ Seq B: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  (9/32 used) â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                                 â”‚
â”‚  SOLUTION: PagedAttention (like OS virtual memory)              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”‚
â”‚                                                                 â”‚
â”‚  Block Pool (GPU memory):                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”           â”‚
â”‚  â”‚ B0 â”‚ B1 â”‚ B2 â”‚ B3 â”‚ B4 â”‚ B5 â”‚ B6 â”‚ B7 â”‚ B8 â”‚ B9 â”‚           â”‚
â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ”‚â–ˆâ–ˆâ–ˆâ–ˆâ”‚â–‘â–‘â–‘â–‘â”‚â–ˆâ–ˆâ–ˆâ–ˆâ”‚â–ˆâ–ˆâ–ˆâ–ˆâ”‚â–‘â–‘â–‘â–‘â”‚â–ˆâ–ˆâ–ˆâ–ˆâ”‚â–‘â–‘â–‘â–‘â”‚â–‘â–‘â–‘â–‘â”‚â–‘â–‘â–‘â–‘â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜           â”‚
â”‚    â–²    â–²         â–²    â–²         â–²                              â”‚
â”‚    â”‚    â”‚         â”‚    â”‚         â”‚                              â”‚
â”‚    â””â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚         â”‚                              â”‚
â”‚   Seq A â”‚              â”‚         â”‚                              â”‚
â”‚  (1 block)             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚                         Seq B                                   â”‚
â”‚                      (3 blocks)                                 â”‚
â”‚                                                                 â”‚
â”‚  Block Table (mapping):                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚  â”‚ Seq A     â”‚ [0, 1]                â”‚  Logical â†’ Physical      â”‚
â”‚  â”‚ Seq B     â”‚ [3, 4, 6]             â”‚                          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                                                                 â”‚
â”‚  Benefits:                                                      â”‚
â”‚  â€¢ No wasted memory (blocks allocated on-demand)                â”‚
â”‚  â€¢ Sequences can grow dynamically                               â”‚
â”‚  â€¢ Prefix sharing (same prefix = share blocks!)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"#
}

/// Return the speculative decoding explanation diagram.
pub fn speculative_decoding_diagram() -> &'static str {
    r#"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Speculative Decoding Explained                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  PROBLEM: Large models are slow at autoregressive decoding      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”‚
â”‚                                                                 â”‚
â”‚  Traditional (one token at a time):                             â”‚
â”‚    Context: "The capital of France is"                          â”‚
â”‚    Step 1: [Large Model] â†’ "Paris"    (slow: 100ms)             â”‚
â”‚    Step 2: [Large Model] â†’ "."        (slow: 100ms)             â”‚
â”‚    Step 3: [Large Model] â†’ "It"       (slow: 100ms)             â”‚
â”‚    Total: 300ms for 3 tokens                                    â”‚
â”‚                                                                 â”‚
â”‚  SOLUTION: Draft with small model, verify with large            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€             â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ DRAFT PHASE (Small Model - Qwen3-0.6B)                  â”‚    â”‚
â”‚  â”‚                                                         â”‚    â”‚
â”‚  â”‚  Context â†’ [Draft] â†’ [Draft] â†’ [Draft] â†’ [Draft]        â”‚    â”‚
â”‚  â”‚            "Paris"   "."       "It"      "is"           â”‚    â”‚
â”‚  â”‚                                                         â”‚    â”‚
â”‚  â”‚  Fast: 10ms Ã— 4 = 40ms total                            â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                          â”‚                                      â”‚
â”‚                          â–¼                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ VERIFY PHASE (Large Model - Qwen3-4B)                   â”‚    â”‚
â”‚  â”‚                                                         â”‚    â”‚
â”‚  â”‚  Process all 4 drafts in ONE forward pass               â”‚    â”‚
â”‚  â”‚                                                         â”‚    â”‚
â”‚  â”‚  Position:    0         1        2        3             â”‚    â”‚
â”‚  â”‚  Draft:      "Paris"   "."      "It"     "is"           â”‚    â”‚
â”‚  â”‚  P(target):   0.87     0.72     0.65     0.40           â”‚    â”‚
â”‚  â”‚  P(draft):    0.82     0.70     0.60     0.80           â”‚    â”‚
â”‚  â”‚  Accept?:     âœ“        âœ“        âœ“        âœ— (reject)     â”‚    â”‚
â”‚  â”‚                                                         â”‚    â”‚
â”‚  â”‚  Time: 100ms (same as 1 token)                          â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                 â”‚
â”‚  Result: 3 tokens accepted + 1 bonus token sampled              â”‚
â”‚  Total time: 40ms + 100ms = 140ms for 4 tokens                  â”‚
â”‚  Speedup: 300ms â†’ 140ms = 2.1x faster!                          â”‚
â”‚                                                                 â”‚
â”‚  KEY INSIGHT: Rejection sampling guarantees output matches      â”‚
â”‚  what the large model would have generated alone.               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"#
}

/// Create an act header for narrator mode.
pub fn act_header(act_num: usize, title: &str) -> String {
    format!(
        "\nğŸ¬ ACT {}: {}\nâ”Œ{}â”\nâ”‚ {:59} â”‚\nâ””{}â”˜",
        act_num,
        title,
        "â”€".repeat(61),
        title,
        "â”€".repeat(61)
    )
}

/// Create an insight/tip box.
pub fn insight_box(text: &str, emoji: &str) -> String {
    let lines: Vec<&str> = text.lines().collect();
    let mut result = format!("\n  {emoji} \n");
    for line in lines {
        result.push_str(&format!("     {line}\n"));
    }
    result
}

/// Create a box around text.
pub fn box_text(text: &str, title: &str, width: usize) -> String {
    let lines: Vec<&str> = text.lines().collect();
    let content_width = width - 4;

    let mut result = Vec::new();

    // Top border with optional title
    if !title.is_empty() {
        let title_part = format!(" {title} ");
        let remaining = width - 2 - title_part.len();
        let left = remaining / 2;
        let right = remaining - left;
        result.push(format!(
            "â”Œ{}{}{}â”",
            "â”€".repeat(left),
            title_part,
            "â”€".repeat(right)
        ));
    } else {
        result.push(format!("â”Œ{}â”", "â”€".repeat(width - 2)));
    }

    // Content
    for line in lines {
        let mut remaining = line;
        while remaining.len() > content_width {
            let (chunk, rest) = remaining.split_at(content_width);
            result.push(format!("â”‚ {chunk} â”‚"));
            remaining = rest;
        }
        result.push(format!("â”‚ {remaining:content_width$} â”‚"));
    }

    // Bottom border
    result.push(format!("â””{}â”˜", "â”€".repeat(width - 2)));

    result.join("\n")
}

/// Format tensor operation with shapes.
pub fn format_tensor_operation(
    op_name: &str,
    input_shapes: &[(&str, &[usize])],
    output_shape: (&str, &[usize]),
    formula: Option<&str>,
) -> String {
    let mut lines = vec![format!("  {}:", op_name)];

    // Inputs
    for (name, shape) in input_shapes {
        let shape_str = format!(
            "[{}]",
            shape
                .iter()
                .map(|d| d.to_string())
                .collect::<Vec<_>>()
                .join(", ")
        );
        lines.push(format!("    {name}: {shape_str}"));
    }

    // Arrow
    lines.push("       â†“".to_string());

    // Output
    let (out_name, out_shape) = output_shape;
    let out_shape_str = format!(
        "[{}]",
        out_shape
            .iter()
            .map(|d| d.to_string())
            .collect::<Vec<_>>()
            .join(", ")
    );
    lines.push(format!("    {out_name}: {out_shape_str}"));

    // Formula
    if let Some(f) = formula {
        lines.push(format!("    Formula: {f}"));
    }

    lines.join("\n")
}

/// Format memory statistics.
pub fn format_memory_stats(
    kv_cache_mb: f32,
    total_blocks: usize,
    used_blocks: usize,
    _block_size: usize,
    num_tokens: usize,
) -> String {
    let bar = memory_bar(used_blocks, total_blocks, 15, "", true);
    format!(
        "â”Œâ”€ Memory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n\
         â”‚ KV Cache: {kv_cache_mb:5.1}MB  â”‚\n\
         â”‚ {bar} â”‚\n\
         â”‚ Blocks: {used_blocks}/{total_blocks}   â”‚\n\
         â”‚ Tokens: {num_tokens}       â”‚\n\
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
    )
}

/// Layer-by-layer progress bar.
pub fn layer_progress_bar(current_layer: usize, total_layers: usize, width: usize) -> String {
    let pct = (current_layer + 1) as f32 / total_layers as f32;
    let filled = (pct * width as f32) as usize;
    let empty = width - filled;

    let bar = format!("{}{}", "â–ˆ".repeat(filled), "â–‘".repeat(empty));
    format!("Layer: {} {}/{}", bar, current_layer + 1, total_layers)
}
